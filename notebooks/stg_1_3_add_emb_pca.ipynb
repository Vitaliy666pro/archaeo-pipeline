{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% ------------------ Imports ------------------\n",
    "# import os\n",
    "# import glob\n",
    "# import yaml\n",
    "# import pickle\n",
    "# import itertools\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import polars as pl\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "# from sklearn.neighbors import BallTree\n",
    "# from sklearn.decomposition import PCA\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# # %% ------------------ Functions ------------------\n",
    "# def pick_point(row):\n",
    "#     pt = row.get(\"geometry_point\")\n",
    "#     if isinstance(pt, Point):\n",
    "#         return pt\n",
    "#     return row.geometry.centroid\n",
    "\n",
    "# def build_tile_dataframe(df, crs=\"EPSG:3857\"):\n",
    "#     \"\"\"Converts input DataFrame to GeoDataFrame and ensures geometry_point exists.\"\"\"\n",
    "#     gdf = gpd.GeoDataFrame(df.copy(), geometry=df[\"geometry\"], crs=crs)\n",
    "#     gdf[\"geometry_point\"] = df.apply(pick_point, axis=1)\n",
    "#     return gdf\n",
    "\n",
    "# def load_embedding_metadata(parquet_dir):\n",
    "#     \"\"\"Loads coordinates and IDs from all .parquet files in the given directory.\"\"\"\n",
    "#     parquets = glob.glob(os.path.join(parquet_dir, \"*.parquet\"))\n",
    "#     if not parquets:\n",
    "#         raise FileNotFoundError(f\"No .parquet files found in {parquet_dir}\")\n",
    "\n",
    "#     coords, ids, offsets = [], [], []\n",
    "#     offset = 0\n",
    "\n",
    "#     for path in tqdm(parquets, desc=\"Reading coords from parquet\"):\n",
    "#         part = pl.read_parquet(path, columns=['unique_id','centre_lat','centre_lon']).to_pandas()\n",
    "#         coords.append(np.vstack([part[\"centre_lat\"], part[\"centre_lon\"]]).T)\n",
    "#         ids.append(part[\"unique_id\"].tolist())\n",
    "#         offsets.append((path, offset, offset + len(part)))\n",
    "#         offset += len(part)\n",
    "\n",
    "#     return np.vstack(coords), list(itertools.chain.from_iterable(ids)), offsets\n",
    "\n",
    "# def match_tiles_to_embeddings(gdf, emb_coords, emb_ids):\n",
    "#     \"\"\"Finds the closest embedding point for each tile centroid.\"\"\"\n",
    "#     tile_xy = np.vstack([gdf.geometry_point.y.values, gdf.geometry_point.x.values]).T\n",
    "#     tile_rad = np.radians(tile_xy)\n",
    "#     tree = BallTree(np.radians(emb_coords), metric='haversine')\n",
    "#     dist_rad, idx = tree.query(tile_rad, k=1)\n",
    "#     dist_m = dist_rad[:, 0] * 6_371_000\n",
    "#     gdf[\"match_id\"] = [emb_ids[i] for i in idx[:, 0]]\n",
    "#     gdf[\"dist_to_emb\"] = dist_m\n",
    "#     return gdf\n",
    "\n",
    "# def load_required_embeddings(needed_ids, file_offsets, emb_ids_flat):\n",
    "#     \"\"\"Loads only those embedding vectors that match tile centroids.\"\"\"\n",
    "#     emb_vectors = {}\n",
    "#     emb_cols = None\n",
    "\n",
    "#     for path, start, end in tqdm(file_offsets, desc=\"Loading embeddings blocks\"):\n",
    "#         block_ids = emb_ids_flat[start:end]\n",
    "#         want = needed_ids.intersection(block_ids)\n",
    "#         if not want:\n",
    "#             continue\n",
    "\n",
    "#         part_pl = (\n",
    "#             pl.read_parquet(path)\n",
    "#               .filter(pl.col(\"unique_id\").is_in(list(want)))\n",
    "#               .select([\"unique_id\", \"embedding\"])\n",
    "#         )\n",
    "#         part = part_pl.to_pandas()\n",
    "#         mat = np.vstack(part[\"embedding\"].values)\n",
    "#         cols = [f\"emb_{i}\" for i in range(mat.shape[1])]\n",
    "#         if emb_cols is None:\n",
    "#             emb_cols = cols\n",
    "\n",
    "#         df_emb = pd.DataFrame(mat, columns=cols, index=part.index)\n",
    "#         df_part = pd.concat([part[[\"unique_id\"]], df_emb], axis=1)\n",
    "\n",
    "#         for _, row in df_part.iterrows():\n",
    "#             uid = row[\"unique_id\"]\n",
    "#             emb_vectors[uid] = {c: row[c] for c in cols}\n",
    "\n",
    "#     return emb_vectors, emb_cols\n",
    "\n",
    "# def attach_embeddings(gdf, emb_vectors, emb_cols):\n",
    "#     \"\"\"Maps embedding values to each tile based on matched embedding ID.\"\"\"\n",
    "#     for c in emb_cols:\n",
    "#         gdf[c] = gdf[\"match_id\"].map(lambda uid: emb_vectors.get(uid, {}).get(c, np.nan))\n",
    "#     return gdf\n",
    "\n",
    "# def add_embeddings_to_tiles(df, parquet_dir):\n",
    "#     \"\"\"Full pipeline: match tiles to embeddings and attach vectors.\"\"\"\n",
    "#     gdf = build_tile_dataframe(df)\n",
    "#     emb_coords, emb_ids_flat, file_offsets = load_embedding_metadata(parquet_dir)\n",
    "#     gdf = match_tiles_to_embeddings(gdf, emb_coords, emb_ids_flat)\n",
    "#     needed_ids = set(gdf[\"match_id\"])\n",
    "#     emb_vectors, emb_cols = load_required_embeddings(needed_ids, file_offsets, emb_ids_flat)\n",
    "#     gdf = attach_embeddings(gdf, emb_vectors, emb_cols)\n",
    "#     return gdf, emb_cols\n",
    "\n",
    "# # %% ------------------ Main ------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load config.yaml\n",
    "#     with open(\"config.yaml\", \"r\") as f:\n",
    "#         config = yaml.safe_load(f)\n",
    "\n",
    "#     processed_path = config[\"processed_data_dir\"]\n",
    "#     parquet_dir    = config[\"embedding_parquet_dir\"]\n",
    "#     out_path       = os.path.join(processed_path, \"all_tiles_features_with_emb.pkl\")\n",
    "\n",
    "#     # Load input data (tiles with soil features)\n",
    "#     input_path = os.path.join(processed_path, \"all_tiles_features_with_soil.pkl\")\n",
    "#     with open(input_path, \"rb\") as f:\n",
    "#         df = pickle.load(f)\n",
    "\n",
    "#     # Add embeddings\n",
    "#     gdf_tiles, emb_cols = add_embeddings_to_tiles(df, parquet_dir)\n",
    "\n",
    "#     # PCA on embeddings\n",
    "#     gdf_tiles[emb_cols] = gdf_tiles[emb_cols].astype(\"float16\")\n",
    "#     gdf_tiles[\"has_geoglyph\"] = gdf_tiles[\"has_geoglyph\"].astype(int)\n",
    "#     X = gdf_tiles[emb_cols].fillna(0).to_numpy()\n",
    "#     pca = PCA(n_components=7, random_state=42)\n",
    "#     pcs = pca.fit_transform(X)\n",
    "#     for i in range(pcs.shape[1]):\n",
    "#         gdf_tiles[f\"PC{i+1}\"] = pcs[:, i]\n",
    "#     gdf_tiles = gdf_tiles.copy()  # Defragment the frame to improve performance\n",
    "\n",
    "#     # Final column selection\n",
    "#     base_cols = [\n",
    "#         \"tile_id\", \"n_geoglyphs\", \"has_geoglyph\",\n",
    "#         \"mean_elev_m\", \"mean_slope_deg\", \"geometry_point\",\n",
    "#         \"is_mountain\", \"dist_to_mountain_m\", \"dist_to_river_m\",\n",
    "#         \"country\", \n",
    "##\"ord_flow\", \"upland_skm\", \n",
    "#           \"drainage_density_m\",\n",
    "#         \"tile_area_km2\", \"drainage_density\", \"tri\", \"twi\",\n",
    "#         \"curv_plan\", \"curv_prof\", \"geometry\", \"coordinates\",\n",
    "#         \"civil\", \"type\", \"source\", \"bbox\", \"longitude\", \"latitude\",\n",
    "#         \"region\", \"clay_0_5cm\", \"ph_h2o_0_5cm\", \"soc_0_5cm\"\n",
    "#     ]\n",
    "#     pc_cols = [f\"PC{i}\" for i in range(1, 8)]\n",
    "#     all_cols = base_cols + pc_cols\n",
    "#     missing = [c for c in all_cols if c not in gdf_tiles.columns]\n",
    "#     if missing:\n",
    "#         raise KeyError(f\"Missing expected columns in gdf_tiles: {missing}\")\n",
    "\n",
    "#     # Save result\n",
    "#     tiles_reduced = gdf_tiles[all_cols].copy()\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(tiles_reduced, f)\n",
    "\n",
    "#     print(f\"[✓] Saved {tiles_reduced.shape[1]} columns × {tiles_reduced.shape[0]} rows to:\\n  {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(input_path, \"rb\") as f:\n",
    "#     tiles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/archaeo/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reading coords from parquet: 100%|██████████| 23/23 [00:02<00:00,  9.47it/s]\n",
      "Loading embeddings blocks: 100%|██████████| 23/23 [02:00<00:00,  5.25s/it]\n",
      "Building embedding DataFrame: 100%|██████████| 1859/1859 [00:00<00:00, 1940087.37it/s]\n",
      "/opt/conda/envs/archaeo/lib/python3.9/site-packages/geopandas/geodataframe.py:1819: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n",
      "/opt/conda/envs/archaeo/lib/python3.9/site-packages/geopandas/geodataframe.py:1819: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n",
      "/opt/conda/envs/archaeo/lib/python3.9/site-packages/geopandas/geodataframe.py:1819: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n",
      "/opt/conda/envs/archaeo/lib/python3.9/site-packages/geopandas/geodataframe.py:1819: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n",
      "/opt/conda/envs/archaeo/lib/python3.9/site-packages/geopandas/geodataframe.py:1819: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n",
      "/opt/conda/envs/archaeo/lib/python3.9/site-packages/geopandas/geodataframe.py:1819: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n",
      "/opt/conda/envs/archaeo/lib/python3.9/site-packages/geopandas/geodataframe.py:1819: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Missing expected columns in gdf_tiles: ['civil', 'type']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 144\u001b[0m\n\u001b[1;32m    142\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m all_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m gdf_tiles\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing expected columns in gdf_tiles: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m tiles_reduced \u001b[38;5;241m=\u001b[39m gdf_tiles[all_cols]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Missing expected columns in gdf_tiles: ['civil', 'type']\""
     ]
    }
   ],
   "source": [
    "# %% ------------------ Imports ------------------\n",
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# %% ------------------ Functions ------------------\n",
    "def pick_point(row):\n",
    "    pt = row.get(\"geometry_point\")\n",
    "    if isinstance(pt, Point):\n",
    "        return pt\n",
    "    return row.geometry.centroid\n",
    "\n",
    "def build_tile_dataframe(df, crs=\"EPSG:3857\"):\n",
    "    gdf = gpd.GeoDataFrame(df.copy(), geometry=df[\"geometry\"], crs=crs)\n",
    "    gdf[\"geometry_point\"] = df.apply(pick_point, axis=1)\n",
    "    return gdf\n",
    "\n",
    "def load_embedding_metadata(parquet_dir):\n",
    "    parquets = glob.glob(os.path.join(parquet_dir, \"*.parquet\"))\n",
    "    if not parquets:\n",
    "        raise FileNotFoundError(f\"No .parquet files found in {parquet_dir}\")\n",
    "\n",
    "    coords, ids, offsets = [], [], []\n",
    "    offset = 0\n",
    "\n",
    "    for path in tqdm(parquets, desc=\"Reading coords from parquet\"):\n",
    "        part = pl.read_parquet(path, columns=['unique_id','centre_lat','centre_lon']).to_pandas()\n",
    "        coords.append(np.vstack([part[\"centre_lat\"], part[\"centre_lon\"]]).T)\n",
    "        ids.append(part[\"unique_id\"].tolist())\n",
    "        offsets.append((path, offset, offset + len(part)))\n",
    "        offset += len(part)\n",
    "\n",
    "    return np.vstack(coords), list(itertools.chain.from_iterable(ids)), offsets\n",
    "\n",
    "def match_tiles_to_embeddings(gdf, emb_coords, emb_ids):\n",
    "    gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    tile_xy = np.vstack([gdf.geometry_point.y.values, gdf.geometry_point.x.values]).T\n",
    "    tile_rad = np.radians(tile_xy)\n",
    "    tree = BallTree(np.radians(emb_coords), metric='haversine')\n",
    "    dist_rad, idx = tree.query(tile_rad, k=1)\n",
    "    dist_m = dist_rad[:, 0] * 6_371_000\n",
    "    gdf[\"match_id\"] = [emb_ids[i] for i in idx[:, 0]]\n",
    "    gdf[\"dist_to_emb\"] = dist_m\n",
    "    return gdf\n",
    "\n",
    "def load_required_embeddings(needed_ids, file_offsets, emb_ids_flat):\n",
    "    emb_vectors = {}\n",
    "    emb_cols = None\n",
    "\n",
    "    for path, start, end in tqdm(file_offsets, desc=\"Loading embeddings blocks\"):\n",
    "        block_ids = emb_ids_flat[start:end]\n",
    "        want = needed_ids.intersection(block_ids)\n",
    "        if not want:\n",
    "            continue\n",
    "\n",
    "        part_pl = (\n",
    "            pl.read_parquet(path)\n",
    "              .filter(pl.col(\"unique_id\").is_in(list(want)))\n",
    "              .select([\"unique_id\", \"embedding\"])\n",
    "        )\n",
    "        part = part_pl.to_pandas()\n",
    "        mat = np.vstack(part[\"embedding\"].values)\n",
    "        cols = [f\"emb_{i}\" for i in range(mat.shape[1])]\n",
    "        if emb_cols is None:\n",
    "            emb_cols = cols\n",
    "\n",
    "        df_emb = pd.DataFrame(mat, columns=cols)\n",
    "        df_emb.insert(0, \"unique_id\", part[\"unique_id\"].values)\n",
    "\n",
    "        for _, row in df_emb.iterrows():\n",
    "            emb_vectors[row[\"unique_id\"]] = {c: row[c] for c in cols}\n",
    "\n",
    "    return emb_vectors, emb_cols\n",
    "\n",
    "def attach_embeddings(gdf, emb_vectors, emb_cols):\n",
    "    emb_items = list(emb_vectors.items())\n",
    "    emb_df = pd.DataFrame(\n",
    "        [v for _, v in tqdm(emb_items, desc=\"Building embedding DataFrame\")],\n",
    "        index=[k for k, _ in emb_items]\n",
    "    )\n",
    "    emb_df.index.name = \"match_id\"\n",
    "    gdf = gdf.merge(emb_df, how=\"left\", left_on=\"match_id\", right_index=True)\n",
    "    return gdf\n",
    "\n",
    "def add_embeddings_to_tiles(df, parquet_dir):\n",
    "    gdf = build_tile_dataframe(df)\n",
    "    emb_coords, emb_ids_flat, file_offsets = load_embedding_metadata(parquet_dir)\n",
    "    gdf = match_tiles_to_embeddings(gdf, emb_coords, emb_ids_flat)\n",
    "    needed_ids = set(gdf[\"match_id\"])\n",
    "    emb_vectors, emb_cols = load_required_embeddings(needed_ids, file_offsets, emb_ids_flat)\n",
    "    gdf = attach_embeddings(gdf, emb_vectors, emb_cols)\n",
    "    return gdf, emb_cols\n",
    "\n",
    "# %% ------------------ Main ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    processed_path = config[\"processed_data_dir\"]\n",
    "    parquet_dir    = config[\"embedding_parquet_dir\"]\n",
    "    out_path       = os.path.join(processed_path, \"all_tiles_features_with_emb.pkl\")\n",
    "\n",
    "    input_path = os.path.join(processed_path, \"all_tiles_features_with_soil.pkl\")\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "\n",
    "    gdf_tiles, emb_cols = add_embeddings_to_tiles(df, parquet_dir)\n",
    "\n",
    "    gdf_tiles[emb_cols] = gdf_tiles[emb_cols].astype(\"float16\")\n",
    "    gdf_tiles[\"has_geoglyph\"] = gdf_tiles[\"has_geoglyph\"].astype(int)\n",
    "    X = gdf_tiles[emb_cols].fillna(0).to_numpy()\n",
    "    pca = PCA(n_components=7, random_state=42)\n",
    "    pcs = pca.fit_transform(X)\n",
    "    for i in range(pcs.shape[1]):\n",
    "        gdf_tiles[f\"PC{i+1}\"] = pcs[:, i]\n",
    "    gdf_tiles = gdf_tiles.copy()\n",
    "\n",
    "    base_cols = [\n",
    "        \"tile_id\", \"n_geoglyphs\", \"has_geoglyph\",\n",
    "        \"mean_elev_m\", \"mean_slope_deg\", \"geometry_point\",\n",
    "        \"is_mountain\", \"dist_to_mountain_m\", \"dist_to_river_m\",\n",
    "        \"country\",\n",
    "         #\"ord_flow\", \"upland_skm\",\n",
    "          \"drainage_density_m\",\n",
    "        \"tile_area_km2\", \"drainage_density\", \"tri\", \"twi\",\n",
    "        \"curv_plan\", \"curv_prof\", \"geometry\", \"coordinates\",\n",
    "        #\"civil\", \"type\", \n",
    "        \"source\", \"bbox\", \"longitude\", \"latitude\",\n",
    "        \"region\", \"clay_0_5cm\", \"ph_h2o_0_5cm\", \"soc_0_5cm\"\n",
    "    ]\n",
    "    pc_cols = [f\"PC{i}\" for i in range(1, 8)]\n",
    "    all_cols = base_cols + pc_cols\n",
    "    missing = [c for c in all_cols if c not in gdf_tiles.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing expected columns in gdf_tiles: {missing}\")\n",
    "\n",
    "    tiles_reduced = gdf_tiles[all_cols].copy()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(tiles_reduced, f)\n",
    "\n",
    "    print(f\"[✓] Saved {tiles_reduced.shape[1]} columns × {tiles_reduced.shape[0]} rows to:\\n  {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Archaeo-Pipeline)",
   "language": "python",
   "name": "archaeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
