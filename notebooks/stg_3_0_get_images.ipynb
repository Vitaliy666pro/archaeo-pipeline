{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 11:19:25,381 INFO Found 30 entries across 18 prefixes (max 3 each)\n",
      "2025-06-29 11:19:25,382 INFO Processing tile ANT_A01_2011_laz_3\n",
      "/tmp/ipykernel_17/4035491276.py:134: RuntimeWarning: invalid value encountered in cast\n",
      "  return (np.clip(hs, 0, 1) * 255).astype(\"uint8\")\n",
      "2025-06-29 11:19:36,303 INFO Processing tile ANTL5770C8970_cleaned\n",
      "2025-06-29 11:19:48,404 INFO Processing tile ANT_A01_2011_laz_5\n",
      "2025-06-29 11:19:58,279 INFO Processing tile ANTL5780C8970_cleaned\n",
      "2025-06-29 11:20:08,421 INFO Processing tile ANTL5790C8970_cleaned\n",
      "2025-06-29 11:20:18,552 INFO Processing tile ANT_A01_2011_laz_8\n",
      "2025-06-29 11:20:26,821 INFO Processing tile ANTL5800C8970_cleaned\n",
      "2025-06-29 11:20:35,893 INFO Processing tile ANTL5770C8971_cleaned\n",
      "2025-06-29 11:20:45,551 INFO Processing tile ANTL5780C8971_cleaned\n",
      "2025-06-29 11:20:55,872 INFO Processing tile ANTL5790C8971_cleaned\n",
      "2025-06-29 11:21:06,577 INFO Processing tile ANTL5800C8971_cleaned\n",
      "2025-06-29 11:21:17,222 INFO Processing tile ANTL5770C8972_cleaned\n",
      "2025-06-29 11:21:25,340 INFO Processing tile ANTL5780C8972_cleaned\n",
      "2025-06-29 11:21:34,892 INFO Processing tile ANTL5790C8972_cleaned\n",
      "2025-06-29 11:21:44,198 INFO Processing tile ANTL5800C8972_cleaned\n",
      "2025-06-29 11:21:57,009 INFO Processing tile TAL_A01_2013_laz_2\n",
      "2025-06-29 11:22:06,530 INFO Processing tile TAL_A01_2013_laz_5\n",
      "2025-06-29 11:22:16,080 INFO Processing tile TAL_A01_2013_laz_7\n",
      "2025-06-29 11:22:24,640 INFO Processing tile JAM_A01_2011_laz_1\n",
      "2025-06-29 11:22:33,994 INFO Processing tile JAM_A01_2011_laz_3\n",
      "2025-06-29 11:22:41,425 INFO Processing tile JAM_A01_2011_laz_4\n",
      "2025-06-29 11:22:50,958 INFO Processing tile FN3_A01_2014_P05_laz_3\n",
      "2025-06-29 11:22:59,144 INFO Processing tile FN3_A01_2014_P05_laz_2\n",
      "2025-06-29 11:23:07,171 INFO Processing tile FN3_A01_2014_P05_laz_1\n",
      "2025-06-29 11:23:15,756 INFO Processing tile BA3_A01_2014_laz_1\n",
      "2025-06-29 11:23:23,492 INFO Processing tile BA3_A01_2014_laz_0\n",
      "2025-06-29 11:23:33,730 INFO Processing tile BA3_A01_2014_laz_8\n",
      "2025-06-29 11:23:41,104 INFO Processing tile ST3_A01_2018_P02_LAS_4\n",
      "2025-06-29 11:23:49,272 INFO Processing tile ST3_A01_2014_P02_laz_8\n",
      "2025-06-29 11:23:58,199 INFO Processing tile ST3_A01_2014_P02_laz_9\n",
      "2025-06-29 11:24:06,998 INFO Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import logging\n",
    "import ee\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from ee import ServiceAccountCredentials\n",
    "\n",
    "# ─────────────────────────── Load environment variables ───────────────────────────\n",
    "load_dotenv()\n",
    "\n",
    "# ─────────────────────────── Load configuration from config.yaml ───────────────────────────\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "RAW_DATA_DIR = Path(config[\"raw_data_dir\"])\n",
    "RESULTS_DIR  = Path(config[\"results_dir\"])\n",
    "\n",
    "# Directory containing the LiDAR DTM tiles\n",
    "DTM_DIR = (\n",
    "    RAW_DATA_DIR\n",
    "    / \"datasets\"\n",
    "    / \"nasa-amazon-lidar-2008-2018\"\n",
    "    / \"Nasa_lidar_2008_to_2018_DTMs\"\n",
    "    / \"DTM_tiles\"\n",
    ")\n",
    "\n",
    "# CSV file listing the candidate tiles to process\n",
    "PATH_TO_COORDS_CSV = RESULTS_DIR / \"candidates_top500.csv\"\n",
    "\n",
    "# Output directory for predicted results\n",
    "OUT_DIR = RESULTS_DIR / \"predicted\"\n",
    "# Create output directory if it doesn't exist\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ─────────────────────────── Earth Engine authentication ───────────────────────────\n",
    "GSA_EMAIL_VAR = os.getenv(\"GSA_EMAIL\")\n",
    "KEY_PATH_VAR  = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
    "\n",
    "creds = ServiceAccountCredentials(GSA_EMAIL_VAR, KEY_PATH_VAR)\n",
    "ee.Initialize(credentials=creds, project=os.getenv(\"GEE_PROJECT\", \"kaggle-ai-to-z\"))\n",
    "\n",
    "# ─────────────────────────── Processing parameters ───────────────────────────\n",
    "MAX_PHOTOS_PER_PREFIX = 3       # Max entries per prefix\n",
    "BUFFER_RADIUS_METERS  = 1500    # Radius for buffering points (meters)\n",
    "DATE_TO_DOWNLOAD      = \"2025-05-01\"\n",
    "AZ1, ALT1             = 315, 45  # Azimuth and altitude for first hillshade\n",
    "AZ2, ALT2             = 45, 30   # Azimuth and altitude for second hillshade\n",
    "\n",
    "# Sentinel-1 and Sentinel-2 collection and visualization parameters\n",
    "EE_COLLECTION_S1 = \"COPERNICUS/S1_GRD\"\n",
    "VIS_PARAMS_S1    = {'bands': ['VV'], 'min': -25, 'max': 5}\n",
    "EE_COLLECTION_S2 = \"COPERNICUS/S2_SR_HARMONIZED\"\n",
    "VIS_PARAMS_S2    = {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000, 'gamma': 1.3}\n",
    "\n",
    "# Ensure the CSV exists\n",
    "if not PATH_TO_COORDS_CSV.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {PATH_TO_COORDS_CSV}\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "\n",
    "# ─────────────────────────── Helper functions ───────────────────────────\n",
    "def get_best_s1(pt, start, end):\n",
    "    \"\"\"\n",
    "    Fetch the earliest Sentinel-1 image in the given date range for the point.\n",
    "    Returns the image and its bounding geometry.\n",
    "    \"\"\"\n",
    "    geom = ee.Geometry.Point(pt).buffer(BUFFER_RADIUS_METERS).bounds()\n",
    "    img  = (\n",
    "        ee.ImageCollection(EE_COLLECTION_S1)\n",
    "        .filterBounds(ee.Geometry.Point(pt))\n",
    "        .filterDate(start, end)\n",
    "        .sort('system:time_start')\n",
    "        .first()\n",
    "    )\n",
    "    return img, geom\n",
    "\n",
    "def get_best_s2(pt, start, end, max_cloud=30):\n",
    "    \"\"\"\n",
    "    Fetch the least cloudy Sentinel-2 image in the given date range for the point.\n",
    "    Returns the image and its bounding geometry.\n",
    "    \"\"\"\n",
    "    geom = ee.Geometry.Point(pt).buffer(BUFFER_RADIUS_METERS).bounds()\n",
    "    img  = (\n",
    "        ee.ImageCollection(EE_COLLECTION_S2)\n",
    "        .filterBounds(ee.Geometry.Point(pt))\n",
    "        .filterDate(start, end)\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', max_cloud))\n",
    "        .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
    "        .first()\n",
    "    )\n",
    "    return img, geom\n",
    "\n",
    "def save_jpg(img, region, out_path, vis_params):\n",
    "    \"\"\"\n",
    "    Save a visualized Earth Engine image as JPEG.\n",
    "    \"\"\"\n",
    "    vis = img.visualize(**vis_params)\n",
    "    url = vis.getThumbURL({'region': region, 'dimensions': 800, 'format': 'jpg'})\n",
    "    data = urllib.request.urlopen(url).read()\n",
    "    out_path.write_bytes(data)\n",
    "\n",
    "def save_tif(img, region, out_path, bands=None):\n",
    "    \"\"\"\n",
    "    Save an Earth Engine image as GeoTIFF.\n",
    "    \"\"\"\n",
    "    params = {'scale': 10, 'region': region, 'format': 'GEO_TIFF', 'crs': 'EPSG:4326'}\n",
    "    if bands:\n",
    "        params['bands'] = bands\n",
    "    url  = img.getDownloadURL(params)\n",
    "    data = urllib.request.urlopen(url).read()\n",
    "    out_path.write_bytes(data)\n",
    "\n",
    "def hillshade(arr, az, alt):\n",
    "    \"\"\"\n",
    "    Compute hillshade from a single-band array given sun azimuth and altitude.\n",
    "    \"\"\"\n",
    "    az, alt = np.deg2rad([az, alt])\n",
    "    dy, dx  = np.gradient(arr.astype(\"float32\"), edge_order=2)\n",
    "    slope   = np.arctan(np.hypot(dx, dy))\n",
    "    aspect  = np.arctan2(dy, -dx)\n",
    "    hs = (np.sin(alt) * np.cos(slope) +\n",
    "          np.cos(alt) * np.sin(slope) * np.cos(az - aspect))\n",
    "    return (np.clip(hs, 0, 1) * 255).astype(\"uint8\")\n",
    "\n",
    "# ─────────────────────────── Main processing ───────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    # Clear the output directory before running\n",
    "    if OUT_DIR.exists():\n",
    "        shutil.rmtree(OUT_DIR)\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    matches = []\n",
    "    prefix_counts = {}\n",
    "\n",
    "    # Read candidate tiles from CSV and limit by prefix\n",
    "    with open(PATH_TO_COORDS_CSV, newline='', encoding='utf-8') as cf:\n",
    "        reader = csv.DictReader(cf)\n",
    "        for row in reader:\n",
    "            full_stem = Path(row[\"filename\"]).stem\n",
    "            prefix    = \"_\".join(full_stem.split(\"_\")[:2])\n",
    "            cnt       = prefix_counts.get(prefix, 0)\n",
    "            if cnt >= MAX_PHOTOS_PER_PREFIX:\n",
    "                continue\n",
    "            prefix_counts[prefix] = cnt + 1\n",
    "\n",
    "            # Compute center coordinates of the tile\n",
    "            minx, miny = float(row[\"min_lon\"]), float(row[\"min_lat\"])\n",
    "            maxx, maxy = float(row[\"max_lon\"]), float(row[\"max_lat\"])\n",
    "            center     = [(minx + maxx) / 2, (miny + maxy) / 2]\n",
    "            matches.append((full_stem, center))\n",
    "\n",
    "    logging.info(\n",
    "        f\"Found {len(matches)} entries across {len(prefix_counts)} prefixes (max {MAX_PHOTOS_PER_PREFIX} each)\"\n",
    "    )\n",
    "\n",
    "    # Process each tile\n",
    "    for stem, pt in matches:\n",
    "        logging.info(f\"Processing tile {stem}\")\n",
    "        tile_dir = OUT_DIR / stem\n",
    "        tile_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Copy LiDAR DTM file\n",
    "        lidar_fp = DTM_DIR / f\"{stem}.tif\"\n",
    "        if not lidar_fp.exists():\n",
    "            logging.warning(f\"LiDAR TIFF missing for {stem}\")\n",
    "            continue\n",
    "        shutil.copy(lidar_fp, tile_dir / f\"{stem}_lidar.tif\")\n",
    "\n",
    "        # Define date range and region for Earth Engine\n",
    "        start  = \"2024-01-01\"\n",
    "        end    = f\"{DATE_TO_DOWNLOAD}T23:59:59\"\n",
    "        region = ee.Geometry.Point(pt).buffer(BUFFER_RADIUS_METERS).bounds()\n",
    "\n",
    "        # Fetch and save Sentinel-1 imagery\n",
    "        s1_img, s1_reg = get_best_s1(pt, start, end)\n",
    "        if s1_img:\n",
    "            save_jpg(s1_img, s1_reg, tile_dir / f\"{stem}_S1_{DATE_TO_DOWNLOAD}.jpg\", VIS_PARAMS_S1)\n",
    "            save_tif(s1_img, s1_reg, tile_dir / f\"{stem}_S1_{DATE_TO_DOWNLOAD}.tif\")\n",
    "        else:\n",
    "            logging.warning(f\"No S1 imagery for {stem}\")\n",
    "\n",
    "        # Fetch and save Sentinel-2 imagery\n",
    "        s2_img, s2_reg = get_best_s2(pt, start, end)\n",
    "        if s2_img:\n",
    "            save_jpg(s2_img, s2_reg, tile_dir / f\"{stem}_S2_{DATE_TO_DOWNLOAD}.jpg\", VIS_PARAMS_S2)\n",
    "            save_tif(s2_img, s2_reg, tile_dir / f\"{stem}_S2_{DATE_TO_DOWNLOAD}.tif\", bands=['B4', 'B3', 'B2'])\n",
    "        else:\n",
    "            logging.warning(f\"No S2 imagery for {stem}\")\n",
    "\n",
    "        # Compute and save hillshade composite from LiDAR\n",
    "        with rasterio.open(lidar_fp) as src:\n",
    "            arr = src.read(1).astype(\"float32\")\n",
    "            nod = src.nodata\n",
    "            if nod is not None:\n",
    "                arr[arr == nod] = np.nan\n",
    "        hs1 = hillshade(arr, AZ1, ALT1)\n",
    "        hs2 = hillshade(arr, AZ2, ALT2)\n",
    "        comp = np.concatenate([hs1, hs2], axis=1)\n",
    "        Image.fromarray(comp).save(tile_dir / f\"{stem}_lidar_hillshade.jpg\", quality=90)\n",
    "\n",
    "    logging.info(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Archaeo-Pipeline)",
   "language": "python",
   "name": "archaeo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
